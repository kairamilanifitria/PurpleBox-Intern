{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tEsgJCDrqssx",
        "gEoWox84qhAx",
        "JeXEEKrgUjRd",
        "DtMyP9xnqzV5"
      ],
      "gpuType": "T4",
      "mount_file_id": "1D4odJjMK-7ABSBWoy4JucdW1almXrx-Y",
      "authorship_tag": "ABX9TyPoBIl3THopZJ8fWLloaG0J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/PurpleBox-Intern/blob/main/03_13_RETRIEVAL_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON - EMBEDDING SUPABASE"
      ],
      "metadata": {
        "id": "tEsgJCDrqssx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load Markdown file\n",
        "file_path = \"/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT.md\"\n",
        "file_name = os.path.basename(file_path)\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    markdown_text = file.read()\n",
        "\n",
        "# Function to check if a chunk contains a Markdown table\n",
        "def is_table(chunk):\n",
        "    return bool(re.search(r'^\\|.*\\|\\n\\|[-| ]+\\|\\n(\\|.*\\|\\n)*', chunk, re.MULTILINE))\n",
        "\n",
        "# Function to extract and split long tables\n",
        "def extract_and_split_table(chunk, max_rows=10):\n",
        "    lines = chunk.strip().split(\"\\n\")\n",
        "    header, table_rows = None, []\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.match(r'^\\|[-| ]+\\|$', line):\n",
        "            header = lines[i - 1].strip(\"|\").split(\"|\")\n",
        "            header = [h.strip() for h in header]\n",
        "            continue\n",
        "        if header:\n",
        "            row_data = line.strip(\"|\").split(\"|\")\n",
        "            row_data = [cell.strip() for cell in row_data]\n",
        "            table_rows.append(row_data)\n",
        "\n",
        "    # Split table into chunks if too many rows\n",
        "    table_chunks = []\n",
        "    for i in range(0, len(table_rows), max_rows):\n",
        "        chunk_rows = table_rows[i:i + max_rows]\n",
        "        table_chunks.append({\"headers\": header, \"rows\": chunk_rows})\n",
        "\n",
        "    return table_chunks if header and table_rows else None\n",
        "\n",
        "# Function to extract section headers\n",
        "def extract_section_title(header):\n",
        "    match = re.match(r'^(#+)\\s+(.*)', header.strip())\n",
        "    return match.group(2) if match else None\n",
        "\n",
        "# Function to detect table title\n",
        "def detect_table_title(pre_table_text):\n",
        "    lines = pre_table_text.strip().split(\"\\n\")\n",
        "    if lines and len(lines[-1].split()) < 10:  # Assuming a title is a short line before a table\n",
        "        return lines[-1]\n",
        "    return None\n",
        "\n",
        "# Function to split text into chunks of max 400 words with 40-word overlap\n",
        "def split_text(text, section_title, max_words=400, overlap=40):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + max_words, len(words))\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        # Prepend section title to first chunk\n",
        "        if start == 0:\n",
        "            chunk = f\"## {section_title}\\n{chunk}\"\n",
        "        chunks.append(chunk)\n",
        "        start += max_words - overlap\n",
        "    return chunks\n",
        "\n",
        "# Process Markdown\n",
        "sections = re.split(r'^(#+\\s+.*)', markdown_text, flags=re.MULTILINE)\n",
        "final_chunks = []\n",
        "current_section = \"Unknown\"\n",
        "chunk_id = 1\n",
        "\n",
        "for i in range(1, len(sections), 2):\n",
        "    section_title = extract_section_title(sections[i]) or current_section\n",
        "    content = sections[i + 1].strip()\n",
        "    current_section = section_title  # Update current section to maintain hierarchy\n",
        "\n",
        "    table_matches = list(re.finditer(r'(\\|.*\\|\\n\\|[-| ]+\\|\\n(?:\\|.*\\|\\n)+)', content, re.MULTILINE))\n",
        "    last_index = 0\n",
        "\n",
        "    for match in table_matches:\n",
        "        start, end = match.span()\n",
        "        pre_table_text = content[last_index:start].strip()\n",
        "        table_text = match.group(0)\n",
        "        last_index = end\n",
        "\n",
        "        table_title = detect_table_title(pre_table_text)  # Extract table title if present\n",
        "        if pre_table_text:\n",
        "            text_chunks = split_text(pre_table_text, section_title)\n",
        "            for chunk in text_chunks:\n",
        "                final_chunks.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"content\": chunk,\n",
        "                    \"metadata\": {\n",
        "                        \"source\": file_name,\n",
        "                        \"section\": section_title,\n",
        "                        \"position\": chunk_id\n",
        "                    }\n",
        "                })\n",
        "                chunk_id += 1\n",
        "\n",
        "        table_chunks = extract_and_split_table(table_text)\n",
        "        if table_chunks:\n",
        "            for table_chunk in table_chunks:\n",
        "                final_chunks.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"table\": table_chunk,\n",
        "                    \"metadata\": {\n",
        "                        \"source\": file_name,\n",
        "                        \"section\": section_title,\n",
        "                        \"table_title\": table_title,\n",
        "                        \"position\": chunk_id\n",
        "                    }\n",
        "                })\n",
        "                chunk_id += 1\n",
        "\n",
        "    remaining_text = content[last_index:].strip()\n",
        "    if remaining_text:\n",
        "        text_chunks = split_text(remaining_text, section_title)\n",
        "        for chunk in text_chunks:\n",
        "            final_chunks.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"content\": chunk,\n",
        "                \"metadata\": {\n",
        "                    \"source\": file_name,\n",
        "                    \"section\": section_title,\n",
        "                    \"position\": chunk_id\n",
        "                }\n",
        "            })\n",
        "            chunk_id += 1\n",
        "\n",
        "# Save JSON output\n",
        "output_file = \"/content/Manuale-IRIS_SLIM_IN_TEC_IT.md.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(final_chunks, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Chunking completed. JSON saved to: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTY0k-hvqfP3",
        "outputId": "317c95ad-758a-405d-f8b9-71cb34bc8c63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking completed. JSON saved to: /content/Manuale-IRIS_SLIM_IN_TEC_IT.md.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase numpy psycopg2"
      ],
      "metadata": {
        "id": "ygLQrkHauqUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import uuid\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize Supabase\n",
        "#SUPABASE_URL = \"_______________\"\n",
        "#SUPABASE_KEY = \"_______________\"\n",
        "SUPABASE_URL = \"\"\n",
        "SUPABASE_KEY = \"\"\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Load Embedding Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ],
      "metadata": {
        "id": "HtTo1NiIwTuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import uuid\n",
        "import torch\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def generate_table_description(table_data, metadata):\n",
        "    \"\"\"Generates a structured description of the table including metadata.\"\"\"\n",
        "    section = metadata.get(\"section\", \"Unknown Section\")\n",
        "    table_title = metadata.get(\"table_title\", \"Unknown Table\")\n",
        "\n",
        "    headers = table_data.get(\"headers\", [])\n",
        "    rows = table_data.get(\"rows\", [])\n",
        "\n",
        "    if len(headers) < 2:\n",
        "        header_key = headers[0] if headers else \"Unknown Header\"\n",
        "        header_value = \"Value\"\n",
        "    else:\n",
        "        header_key, header_value = headers[:2]\n",
        "\n",
        "    row_descriptions = [f\"{header_key}: {row[0]}, {header_value}: {row[1]}\" for row in rows if len(row) >= 2]\n",
        "\n",
        "    description = f\"{section}\\n{table_title}\\n\" + \" | \".join(row_descriptions)\n",
        "    return description\n",
        "\n",
        "def convert_table_to_text(table_data, metadata):\n",
        "    \"\"\"Converts a table (headers + rows) into a structured text format with metadata and description for embedding.\"\"\"\n",
        "    headers = \", \".join(table_data.get(\"headers\", []))\n",
        "    rows = [\" | \".join(row) for row in table_data.get(\"rows\", [])]\n",
        "\n",
        "    # Generate description from table data\n",
        "    table_description = generate_table_description(table_data, metadata)\n",
        "\n",
        "    # Combine metadata with table content\n",
        "    return (\n",
        "        f\"Table Title: {metadata.get('table_title', 'Unknown Table')}. Section: {metadata.get('section', 'Unknown Section')}\\n\"\n",
        "        f\"Table Data:\\nHeaders: {headers}\\n\" + \"\\n\".join(rows) +\n",
        "        f\"\\nDescription: {table_description}\"\n",
        "    ), table_description  # Return both formatted text & natural description\n",
        "\n",
        "def store_chunks_in_supabase(chunks):\n",
        "    \"\"\"Stores text and table chunks into Supabase with improved embeddings.\"\"\"\n",
        "    document_entries = []\n",
        "    table_entries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_id = str(uuid.uuid4())  # Generate unique chunk_id\n",
        "\n",
        "        # Process text content\n",
        "        if \"content\" in chunk and chunk[\"content\"]:\n",
        "            content = chunk[\"content\"]\n",
        "            embedding = get_embedding(content)\n",
        "\n",
        "            document_entries.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"content\": content,\n",
        "                \"embedding\": embedding,\n",
        "                \"metadata\": chunk[\"metadata\"],\n",
        "                \"type\": \"text\"\n",
        "            })\n",
        "\n",
        "        # Process table data\n",
        "        if \"table\" in chunk and chunk[\"table\"]:\n",
        "            table_data = chunk[\"table\"]\n",
        "            metadata = chunk.get(\"metadata\", {})\n",
        "\n",
        "            # Generate both structured table text & natural description\n",
        "            table_text, table_description = convert_table_to_text(table_data, metadata)\n",
        "            table_embedding = get_embedding(table_text)\n",
        "\n",
        "            table_entries.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"table_data\": json.dumps(table_data, ensure_ascii=False),\n",
        "                \"description\": table_description,  # Store the generated description\n",
        "                \"embedding\": table_embedding,\n",
        "                \"metadata\": metadata\n",
        "            })\n",
        "\n",
        "    # Batch insert into Supabase\n",
        "    if document_entries:\n",
        "        supabase.table(\"documents\").insert(document_entries).execute()\n",
        "    if table_entries:\n",
        "        supabase.table(\"tables\").insert(table_entries).execute()"
      ],
      "metadata": {
        "id": "gs90daYUwbz6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load JSON chunks\n",
        "json_file_path = \"/content/Manuale-IRIS_SLIM_IN_TEC_IT.md.json\"\n",
        "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
        "    json_chunks = json.load(json_file)\n",
        "\n",
        "# Store chunks in Supabase\n",
        "store_chunks_in_supabase(json_chunks)\n",
        "print(\"Text and table embeddings stored successfully in Supabase!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yndrwLS4wfwV",
        "outputId": "2f2fb2c4-a27d-49e0-9dde-019efec70e13"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text and table embeddings stored successfully in Supabase!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORKS"
      ],
      "metadata": {
        "id": "gEoWox84qhAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "pZafvZ-bTgyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_keywords_simple(text):\n",
        "    \"\"\"Extracts important words from a query using simple filtering.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def query_requires_table(user_query):\n",
        "    \"\"\"Determines if the query is likely asking for table data.\"\"\"\n",
        "    table_keywords = {\"table\", \"data\", \"values\", \"measurements\", \"limits\", \"thresholds\", \"parameters\"}\n",
        "    return any(word in user_query.lower() for word in table_keywords)\n",
        "\n",
        "def query_supabase(user_query):\n",
        "    \"\"\"Retrieves both text and table chunks based on query, ensuring relevance balance.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "    requires_table = query_requires_table(user_query)\n",
        "    keywords = extract_keywords_simple(user_query)\n",
        "\n",
        "    #### Step 1: Retrieve Text Chunks (Vector Search) ####\n",
        "    response_text = supabase.table(\"documents\").select(\"chunk_id, content, embedding, type, metadata\").execute()\n",
        "    text_results = []\n",
        "\n",
        "    for record in response_text.data:\n",
        "        chunk_embedding = ast.literal_eval(record[\"embedding\"]) if isinstance(record[\"embedding\"], str) else record[\"embedding\"]\n",
        "        chunk_embedding = np.array(chunk_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "        if chunk_embedding.shape == query_embedding.shape:\n",
        "            similarity = 1 - cosine(query_embedding, chunk_embedding)\n",
        "            text_results.append((record[\"chunk_id\"], \"text\", record[\"content\"], similarity))\n",
        "\n",
        "    text_results.sort(key=lambda x: x[3], reverse=True)  # Sort by similarity\n",
        "    top_text_chunks = text_results[:3]  # Keep top 3 text chunks\n",
        "\n",
        "    #### Step 2: Retrieve Table Chunks Using Extracted Keywords ####\n",
        "    response_tables = supabase.table(\"tables\").select(\"chunk_id, table_data, description, embedding, metadata\").execute()\n",
        "    table_results = []\n",
        "\n",
        "    for record in response_tables.data:\n",
        "        table_data = record[\"table_data\"].lower()\n",
        "        table_description = record[\"description\"].lower()\n",
        "        keyword_match_score = sum(1 for word in keywords if word in table_data or word in table_description)\n",
        "\n",
        "        if keyword_match_score > 0:\n",
        "            table_results.append((record[\"chunk_id\"], \"table\", record[\"description\"], keyword_match_score))\n",
        "\n",
        "    table_results.sort(key=lambda x: x[3], reverse=True)  # Sort by keyword relevance\n",
        "\n",
        "    #### Step 3: Merge & Sort Results ####\n",
        "    final_results = text_results[:3] + table_results[:2]  # Ensure text priority, limit tables\n",
        "    final_results.sort(key=lambda x: x[3], reverse=True)  # Sort again by relevance\n",
        "\n",
        "    return final_results[:5]  # Return top 5 most relevant results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcQRaM7VNS8-",
        "outputId": "b4e9a9be-be20-49c3-bd31-d61491ea300c"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# OpenAI API Key\n",
        "OPENAI_API_KEY = \"\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Function to call OpenAI LLM with chat history\n",
        "def call_openai_llm(user_query, retrieved_chunks, chat_history=[]):\n",
        "    \"\"\"Send the query along with retrieved context and chat history to OpenAI API.\"\"\"\n",
        "\n",
        "    # Prepare context from retrieved chunks\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    # Construct messages for conversational memory\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\"},\n",
        "    ]\n",
        "\n",
        "    # Append chat history\n",
        "    messages.extend(chat_history)\n",
        "\n",
        "    # Append current query with retrieved context\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nUser's Question: {user_query}\"})\n",
        "\n",
        "    # Call OpenAI's Chat API with the new format\n",
        "    client = openai.OpenAI(api_key=openai.api_key)  # Ensure you are using the new client-based API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",  # You can change this to another OpenAI model\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content  # Adjusted based on the new API response format\n",
        "\n",
        "    # Append response to chat history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return answer, chat_history"
      ],
      "metadata": {
        "id": "mDMh71-mrP_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"usage limit\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3317fd5-e5d0-4fb8-8aa4-216916d57e51",
        "id": "hL8r8sm-NZrT"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 1.0000\n",
            "\n",
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 1.0000\n",
            "\n",
            "Chunk ID: 8843584e-ca30-4846-b7cf-234dfabb403b\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego...\n",
            "Relevance: 0.8015\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.7723\n",
            "\n",
            "Chunk ID: 61b9d9e2-6e2e-4430-ba82-46680cca3884\n",
            "Type: text\n",
            "Content: ## IRIS SLIM / IN\n",
            "Manuale d'installazione ed uso...\n",
            "Relevance: 0.7599\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the usage limit?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77f5fef-1692-4aef-9eaa-233a63ff5d09",
        "id": "b9DFMBpiNZrT"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 1.0000\n",
            "\n",
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 1.0000\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.8683\n",
            "\n",
            "Chunk ID: 64f46084-3a66-425c-8eb7-86b9ddd27c9c\n",
            "Type: text\n",
            "Content: ## 2.2. Usi Non Previsti E Controindicazioni\n",
            "Non sono ammesse le seguenti applicazioni: - · Funzionamento all'aperto - · Funzionamento in ambienti umidi o esplosivi o polverosi - · Funzionamento in ambienti corrosivi, in particolare per le alette d'alluminio della batteria - · Funzionamento in ambie...\n",
            "Relevance: 0.8667\n",
            "\n",
            "Chunk ID: b7ce279a-5731-4bb6-9f73-274e4e99a2e8\n",
            "Type: text\n",
            "Content: ## 2.1. Uso Previsto\n",
            "Le unità Iris Slim sono progettate per la funzione di riscaldamento, raffrescamento, deumidificazione e filtrazione di ambienti residenziali e terziario (uffici, locali pubblici, o simili)....\n",
            "Relevance: 0.8592\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the Limits of use of the unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711f36ac-83cd-409f-fe5d-7cd955e4a605",
        "id": "tZ8yjlsgWOo9"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The limits of use of the unit, as specified in Chunk 3 (2.2. Usi Non Previsti E Controindicazioni) and Chunk 5 (2.5. Limiti Di Impiego), include:\n",
            "\n",
            "1. **Prohibited Applications**:\n",
            "   - Operation outdoors.\n",
            "   - Operation in humid, explosive, or dusty environments.\n",
            "   - Operation in corrosive environments, particularly harmful for the aluminum fins of the battery.\n",
            "   - Operation in environments with electromagnetic disturbances.\n",
            "\n",
            "2. **User Restrictions**:\n",
            "   - The machine is not intended for use by individuals (including children) with reduced physical, mental, or sensory capacities, or by those who have not received sufficient instruction, unless supervised by someone responsible for their safety.\n",
            "\n",
            "3. **Operational Extremes**:\n",
            "   - It is advised to operate the machine at the extremes of the specified limits of use only for short periods because prolonged operation can reduce the normal lifespan of the components.\n",
            "\n",
            "These limitations are important to ensure the safe and effective operation of the unit and to avoid any potential damage or reduction in component lifespan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the usage limit of the unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa65422-025f-4e35-e9da-b92868870114",
        "id": "y_IPldV6WOo9"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The usage limits of the unit, as specified in Chunk 2 (\"2.5. Limiti Di Impiego\"), are as follows:\n",
            "\n",
            "1. **Electrical Supply:**\n",
            "   - **Water Inlet Temperature for Battery:** 5 - 70 °C when operating at 220 - 240 V / 50 Hz.\n",
            "   - **Air Recovery Temperature:** 10 - 35 °C when operating at 220 - 240 V / 50 Hz.\n",
            "   - **Relative Humidity of Recovered Air:** 10 - 70 % when operating at 220 - 240 V / 50 Hz.\n",
            "\n",
            "Additionally, it is advised in Chunk 4 that the unit should only be operated at these extreme limits for short periods to avoid reducing the normal lifespan of its components.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FkfRqjizUjO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BEST PRACTICE**"
      ],
      "metadata": {
        "id": "JeXEEKrgUjRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "V7t4SaDMVV4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_keywords_simple(text):\n",
        "    \"\"\"Extracts important words from a query using simple filtering.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def query_requires_table(user_query):\n",
        "    \"\"\"Determines if the query is likely asking for table data.\"\"\"\n",
        "    table_keywords = {\"table\", \"data\", \"values\", \"measurements\", \"limits\", \"thresholds\", \"parameters\", \"average\", \"sum\", \"percentage\"}\n",
        "    return any(word in user_query.lower() for word in table_keywords)\n",
        "\n",
        "def get_most_similar_keywords(query_keywords, top_text_chunks):\n",
        "    \"\"\"Extracts most relevant words from top retrieved text chunks.\"\"\"\n",
        "    all_text_words = set()\n",
        "    for chunk in top_text_chunks:\n",
        "        chunk_words = set(word_tokenize(chunk[2].lower()))  # Extract words from chunk text\n",
        "        all_text_words.update(chunk_words)\n",
        "    common_words = [word for word in query_keywords if word in all_text_words]\n",
        "    return common_words if common_words else query_keywords  # Fallback to original keywords if no match\n",
        "\n",
        "def query_supabase(user_query):\n",
        "    \"\"\"Retrieves both text and table chunks based on query, ensuring relevance balance.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "    keywords = extract_keywords_simple(user_query)\n",
        "    requires_table = query_requires_table(user_query)\n",
        "\n",
        "    #### Step 1: Retrieve Text Chunks (Vector Search) ####\n",
        "    response_text = supabase.table(\"documents\").select(\"chunk_id, content, embedding, type, metadata\").execute()\n",
        "    text_results = []\n",
        "\n",
        "    for record in response_text.data:\n",
        "        chunk_embedding = ast.literal_eval(record[\"embedding\"]) if isinstance(record[\"embedding\"], str) else record[\"embedding\"]\n",
        "        chunk_embedding = np.array(chunk_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "        if chunk_embedding.shape == query_embedding.shape:\n",
        "            similarity = 1 - cosine(query_embedding, chunk_embedding)\n",
        "            text_results.append((record[\"chunk_id\"], \"text\", record[\"content\"], similarity))\n",
        "\n",
        "    text_results.sort(key=lambda x: x[3], reverse=True)\n",
        "    top_text_chunks = text_results[:3]\n",
        "\n",
        "    #### Step 2: Expand Query Using Retrieved Text ####\n",
        "    refined_keywords = get_most_similar_keywords(keywords, top_text_chunks)\n",
        "\n",
        "    #### Step 3: Retrieve Table Chunks Using Specialized Scoring ####\n",
        "    response_tables = supabase.table(\"tables\").select(\"chunk_id, table_data, description, embedding, metadata\").execute()\n",
        "    table_results = []\n",
        "    table_weight = 2.5 if requires_table else 1.5  # Increase weight dynamically\n",
        "\n",
        "    for record in response_tables.data:\n",
        "        table_embedding = ast.literal_eval(record[\"embedding\"]) if isinstance(record[\"embedding\"], str) else record[\"embedding\"]\n",
        "        table_embedding = np.array(table_embedding, dtype=np.float32).flatten()\n",
        "        table_data = record[\"table_data\"].lower()\n",
        "        table_description = record[\"description\"].lower()\n",
        "        keyword_match_score = sum(3 if word in table_data.split(\" \")[:5] else 1 for word in refined_keywords if word in table_data or word in table_description)\n",
        "\n",
        "        if table_embedding.shape == query_embedding.shape:\n",
        "            embedding_similarity = 1 - cosine(query_embedding, table_embedding)\n",
        "            keyword_embedding_score = sum(1 - cosine(get_embedding(word), table_embedding) for word in refined_keywords) / max(len(refined_keywords), 1)\n",
        "\n",
        "            final_table_score = (embedding_similarity ** 0.8) * 0.2 + (keyword_match_score ** 2.5) * 0.6 + (keyword_embedding_score ** 1.2) * 0.2\n",
        "\n",
        "            if final_table_score > 0:\n",
        "                table_results.append((record[\"chunk_id\"], \"table\", record[\"description\"], final_table_score))\n",
        "\n",
        "    table_results.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "    #### Step 4: Merge & Rank Results with Adaptive Prioritization ####\n",
        "    if table_results and table_results[0][3] > 0.75:\n",
        "        final_results = [table_results[0]] + text_results[:2] + table_results[1:2] + text_results[2:]\n",
        "    else:\n",
        "        final_results = text_results[:3] + table_results[:2]  # Natural sorting if no table is required\n",
        "\n",
        "    return final_results[:5]  # Return top 5 most relevant results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYeEZR9cNT3_",
        "outputId": "1c6dba9c-d391-4c46-f94a-253c9ed5780e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"usage limit\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df14738-0107-401f-a5fa-1b154b69ef34",
        "id": "4qC7O9VPVNNF"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 0.8898\n",
            "\n",
            "Chunk ID: 8843584e-ca30-4846-b7cf-234dfabb403b\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego...\n",
            "Relevance: 0.8015\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.7723\n",
            "\n",
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 0.8775\n",
            "\n",
            "Chunk ID: 61b9d9e2-6e2e-4430-ba82-46680cca3884\n",
            "Type: text\n",
            "Content: ## IRIS SLIM / IN\n",
            "Manuale d'installazione ed uso...\n",
            "Relevance: 0.7599\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the usage limit?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c045c0-40d5-40c8-ee56-26ed1159145f",
        "id": "3vmGoHjFVNNF"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 0.9077\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.8683\n",
            "\n",
            "Chunk ID: 64f46084-3a66-425c-8eb7-86b9ddd27c9c\n",
            "Type: text\n",
            "Content: ## 2.2. Usi Non Previsti E Controindicazioni\n",
            "Non sono ammesse le seguenti applicazioni: - · Funzionamento all'aperto - · Funzionamento in ambienti umidi o esplosivi o polverosi - · Funzionamento in ambienti corrosivi, in particolare per le alette d'alluminio della batteria - · Funzionamento in ambie...\n",
            "Relevance: 0.8667\n",
            "\n",
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 0.8943\n",
            "\n",
            "Chunk ID: b7ce279a-5731-4bb6-9f73-274e4e99a2e8\n",
            "Type: text\n",
            "Content: ## 2.1. Uso Previsto\n",
            "Le unità Iris Slim sono progettate per la funzione di riscaldamento, raffrescamento, deumidificazione e filtrazione di ambienti residenziali e terziario (uffici, locali pubblici, o simili)....\n",
            "Relevance: 0.8592\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the usage limit of the unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHKoJBqWcknf",
        "outputId": "c737c10c-1cce-4f65-cf4d-f9a0b37c246c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The usage limits of the unit, as outlined in Chunk 1 under \"2.5. Limiti Di Impiego,\" include the following specifications:\n",
            "\n",
            "- **Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz**: 5 - 70 °C\n",
            "- **Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz**: 10 - 35 °C\n",
            "- **Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V / 50 Hz**: 10 - 70 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the dimensions and weight?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXBTwEcJVGw4",
        "outputId": "70ad806f-1c51-4742-84a5-6e2eac48ed18"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 5435d029-60dc-4c33-8ce9-9272abc58495\n",
            "Type: text\n",
            "Content: ## 2.6.1. IRIS Slim Verticale Con Mobile\n",
            "| Peso (kg) | 17 | 20 | 23 | 26 |...\n",
            "Relevance: 0.8080\n",
            "\n",
            "Chunk ID: 6bc39d25-a0e3-4eb5-bb19-671143f300d2\n",
            "Type: text\n",
            "Content: ## 2.6.1. IRIS Slim Verticale Con Mobile\n",
            "Figura 1. Iris slim verticale con mobile ![Image](/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT_artifacts/image_000006_b273c4f4e7016f5303fc41823055cb7b77ee9a415b0e6e68f0edbe8aa520b8a6.png) *Image Description:* The image depicts a sk...\n",
            "Relevance: 0.8072\n",
            "\n",
            "Chunk ID: eb0edfb9-bbde-471a-8c16-8ff1eb336945\n",
            "Type: text\n",
            "Content: ## 2.6.2. IRIS Slim Verticale Da Incasso\n",
            "Figura 3. Controcassa ![Image](/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT_artifacts/image_000007_e4e65540859ec50cd31bba396970d1ffc6d937f61170ddfca4bfc718bb687e96.png) *Image Description:* The image shows two technical drawings of...\n",
            "Relevance: 0.8048\n",
            "\n",
            "Chunk ID: 59bafe85-ef03-4a7a-8c83-97a08aae26c4\n",
            "Type: table\n",
            "Content: 2.6.2. IRIS Slim Verticale Da Incasso\n",
            "Tabella 3. Dimensioni e peso\n",
            "Grandezza: B (mm), 601: 305 | Grandezza: L (mm), 601: 715 | Grandezza: L1 (mm), 601: 795 | Grandezza: Peso (kg), 601: 9...\n",
            "Relevance: 0.3132\n",
            "\n",
            "Chunk ID: 83b3e7ff-bfd9-4290-8267-4a1aee36f4e6\n",
            "Type: table\n",
            "Content: 2.6.1. IRIS Slim Verticale Con Mobile\n",
            "Tabella 2. Dimensioni e peso\n",
            "Grandezza: A (mm), 601: 600...\n",
            "Relevance: 0.3095\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the dimensions and weight of IRIS Slim Vertical With Cabinet?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjrrBSXVcqKV",
        "outputId": "60d52f40-8169-454f-cfe7-e8a84c9fc052"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The dimensions and weight of the IRIS Slim Verticale Con Mobile are as follows:\n",
            "\n",
            "- **Dimension (Height in mm)**: 600\n",
            "- **Weight (kg)**: The weight options available are 17 kg, 20 kg, 23 kg, and 26 kg.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"explain about Installation Arrangements?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik50j_A9bqd4",
        "outputId": "b932aaf5-e350-4d6a-9a1e-71a8fc7bc034"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 40d0a96f-de61-4b5a-abb6-cae3d3cbb5ac\n",
            "Type: text\n",
            "Content: ## 4.1. Predisposizioni All'installazione Di IRIS Slim\n",
            "Fissare l'unità al muro con le quattro viti (in base alle dimensioni delle teste delle viti possono essere necessarie delle rondelle). Al termine dell'installazione l'unità deve risultare perfettamente in orizzontale o con lieve pendenza nella d...\n",
            "Relevance: 0.8430\n",
            "\n",
            "Chunk ID: 96c40b33-dc34-467d-bd8f-bd7bc845d40d\n",
            "Type: text\n",
            "Content: ## 4.2. Posizionamento\n",
            "L'unità deve essere installata a parete che deve essere perfettamente verticale (90° rispetto al pavimento). Rispettare le misure minime riportate in figura, che sono necessarie per una agevole installazione e corretto funzionamento dell'unità. L'unità non deve essere esposta ...\n",
            "Relevance: 0.8334\n",
            "\n",
            "Chunk ID: 07c9674e-5746-445f-8f00-05232d58a6ec\n",
            "Type: text\n",
            "Content: ## 4.1. Predisposizioni All'installazione Di IRIS Slim\n",
            "Forare il muro con gli interassi riportati ed inserire i quattro tasselli nei fori. ![Image](/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT_artifacts/image_000015_909b9a9c01ae0d7a6eccf8010a7a7ea5f483fabec444792d2e5f9880...\n",
            "Relevance: 0.8332\n",
            "\n",
            "Chunk ID: e377dbf0-9b8b-4ae7-aab7-eee8f1466ac1\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Il pannello radiante frontale non si  riscalda, Possibili guasti: Presenza di aria nelle tubazioni | Anomalia: La ventilazione non risponde  immediatamente alle nuove  impostazioni, Possibili guasti: La valvola impiega quale minuto ad  aprirsi | Anomalia: ...\n",
            "Relevance: 0.3059\n",
            "\n",
            "Chunk ID: ed469014-dea5-4506-8c16-52f335750f9d\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: L'apparecchio emette un rumore  eccessivo, Possibili guasti: La ventola non ben fissata al motore | Anomalia: L'apparecchio emette un rumore  eccessivo, Possibili guasti: Lamiere che vibrano | Anomalia: La portata d'aria è molto bassa, Possibili guasti: Fi...\n",
            "Relevance: 0.3037\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"explain about Installation Arrangements\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJI_W56Ac_39",
        "outputId": "92d61575-32d1-4caa-b802-f2c37e5afd54"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The installation arrangements for the IRIS Slim unit, as described in the provided texts, involve several important steps and considerations to ensure proper installation and functionality. Here is a detailed explanation based on the chunks provided:\n",
            "\n",
            "1. **Wall Preparation and Unit Mounting**:\n",
            "   - **Drilling the Wall**: Begin by drilling the wall at specified intervals (interassi), which are likely provided by the technical diagrams in the manual. The holes are meant for inserting dowels (tasselli) that will hold the unit securely.\n",
            "   - **Securing the Unit**: Attach the IRIS Slim unit to the wall using four screws. Depending on the screw head size, washers may be needed to ensure a secure fit. The unit should be perfectly horizontal or slightly tilted toward the condensate drain side. It is crucial to avoid any backward tilt opposite the condensate drain as this can impede the natural flow of the condensate, potentially causing operational issues.\n",
            "\n",
            "2. **Positioning the Unit**: \n",
            "   - **Vertical Alignment**: The wall on which the unit is installed must be perfectly vertical (90° to the floor). This ensures the unit operates correctly and avoids any mechanical strain or misalignment.\n",
            "   - **Avoiding Direct Exposure**: The unit should not be directly exposed to sunlight or other heat sources, which could affect its efficiency and operational lifespan.\n",
            "   - **Room Placement**: Position the unit in a location that allows for even air treatment throughout the room. It should not directly blow air onto occupants, ensuring comfortable and effective air distribution.\n",
            "\n",
            "3. **Technical Considerations**:\n",
            "   - The installation process might be illustrated with technical diagrams, showing dimensions and specific placements for components like control boxes or HVAC elements, ensuring clarity and accuracy in installation.\n",
            "\n",
            "These steps are designed to ensure that the IRIS Slim operates efficiently, with minimal noise and maximum effectiveness in climate control within the space. Proper installation is crucial for the longevity and functionality of the unit. Ensure that all guidelines and local regulations regarding installation are followed to achieve the best results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_0wQnD1WcVXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRIAL ERROR"
      ],
      "metadata": {
        "id": "DtMyP9xnqzV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import deque\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def query_supabase(user_query):\n",
        "    \"\"\"Retrieves both text and table chunks based on query, using improved embeddings and keyword matching.\"\"\"\n",
        "\n",
        "    #### Step 1: Get Query Embedding ####\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "\n",
        "    #### Step 2: Retrieve Text Chunks (Vector Search) ####\n",
        "    response_text = supabase.table(\"documents\").select(\"chunk_id, content, embedding, type, metadata\").execute()\n",
        "    text_results = []\n",
        "\n",
        "    for record in response_text.data:\n",
        "        chunk_embedding = record[\"embedding\"]\n",
        "\n",
        "        # Convert stored string embeddings to list if needed\n",
        "        if isinstance(chunk_embedding, str):\n",
        "            chunk_embedding = ast.literal_eval(chunk_embedding)\n",
        "\n",
        "        chunk_embedding = np.array(chunk_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "        if chunk_embedding.shape == query_embedding.shape:\n",
        "            similarity = 1 - cosine(query_embedding, chunk_embedding)\n",
        "            text_results.append((record[\"chunk_id\"], \"text\", record[\"content\"], similarity))\n",
        "\n",
        "    #### Step 3: Retrieve Table Chunks (Description + Embedding Match) ####\n",
        "    response_tables = supabase.table(\"tables\").select(\"chunk_id, table_data, description, embedding, metadata\").execute()\n",
        "    table_results = []\n",
        "\n",
        "    for record in response_tables.data:\n",
        "        table_data = record[\"table_data\"]\n",
        "        metadata = record.get(\"metadata\", {})\n",
        "        table_description = record.get(\"description\", \"\")  # Use generated description\n",
        "        table_embedding = record.get(\"embedding\", None)\n",
        "\n",
        "        # Ensure metadata fields are strings\n",
        "        table_title = str(metadata.get(\"table_title\", \"\"))\n",
        "        section = str(metadata.get(\"section\", \"\"))\n",
        "\n",
        "        # Extract table number from the query (if any)\n",
        "        table_number_match = re.search(r'table (\\d+)', user_query, re.IGNORECASE)\n",
        "        specified_table_number = table_number_match.group(1) if table_number_match else None\n",
        "\n",
        "        # Step 3.1: Keyword Matching for Table Title, Section & Description\n",
        "        keyword_match_score = 0\n",
        "        if re.search(rf\"\\b{re.escape(user_query)}\\b\", table_title, re.IGNORECASE):\n",
        "            keyword_match_score += 0.5  # Higher weight for title match\n",
        "        if re.search(rf\"\\b{re.escape(user_query)}\\b\", section, re.IGNORECASE):\n",
        "            keyword_match_score += 0.3  # Lower weight for section match\n",
        "        if re.search(rf\"\\b{re.escape(user_query)}\\b\", table_description, re.IGNORECASE):\n",
        "            keyword_match_score += 0.7  # Highest weight for description match\n",
        "\n",
        "        # Prioritize the exact table number if mentioned\n",
        "        if specified_table_number and specified_table_number in table_title.lower():\n",
        "            keyword_match_score += 1.0  # Give a strong boost to matching table numbers\n",
        "\n",
        "        # Step 3.2: Compute Embedding Similarity\n",
        "        if table_embedding:\n",
        "            if isinstance(table_embedding, str):\n",
        "                table_embedding = ast.literal_eval(table_embedding)  # Convert string to list\n",
        "            table_embedding = np.array(table_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "            if table_embedding.shape == query_embedding.shape:\n",
        "                similarity = 1 - cosine(query_embedding, table_embedding)\n",
        "                final_score = (0.7 * similarity) + (1.3 * keyword_match_score)  # Boost keyword matching\n",
        "                table_results.append((record[\"chunk_id\"], \"table\", table_description, final_score))\n",
        "\n",
        "    #### Step 4: Merge & Sort Results ####\n",
        "    all_results = text_results + table_results\n",
        "    all_results.sort(key=lambda x: x[3], reverse=True)  # Sort by final similarity score\n",
        "\n",
        "    return all_results[:5]  # Return top 5 results"
      ],
      "metadata": {
        "id": "SXH1YSviwkik"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oA41plAD_eHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_relevant_words(query, text):\n",
        "    \"\"\"Finds the most relevant words from the retrieved text based on similarity to the query.\"\"\"\n",
        "    query_words = query.lower().split()\n",
        "    text_words = text.lower().split()\n",
        "    word_counts = Counter(text_words)\n",
        "    relevant_words = [word for word in text_words if word in query_words or word_counts[word] > 1]\n",
        "    return list(set(relevant_words))  # Return unique words\n",
        "\n",
        "def query_supabase(user_query):\n",
        "    \"\"\"Retrieves both text and table chunks based on query, ensuring relevance balance.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "\n",
        "    #### Step 1: Retrieve Text Chunks (Vector Search) ####\n",
        "    response_text = supabase.table(\"documents\").select(\"chunk_id, content, embedding, type, metadata\").execute()\n",
        "    text_results = []\n",
        "\n",
        "    for record in response_text.data:\n",
        "        chunk_embedding = record[\"embedding\"]\n",
        "        if isinstance(chunk_embedding, str):\n",
        "            chunk_embedding = ast.literal_eval(chunk_embedding)\n",
        "        chunk_embedding = np.array(chunk_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "        if chunk_embedding.shape == query_embedding.shape:\n",
        "            similarity = 1 - cosine(query_embedding, chunk_embedding)\n",
        "            text_results.append((record[\"chunk_id\"], \"text\", record[\"content\"], similarity))\n",
        "\n",
        "    text_results.sort(key=lambda x: x[3], reverse=True)  # Sort by similarity\n",
        "    top_text_chunks = text_results[:3]  # Keep top 3 text chunks\n",
        "\n",
        "    #### Step 2: Extract Most Relevant Words from Top Text Chunks ####\n",
        "    relevant_words = []\n",
        "    for chunk in top_text_chunks:\n",
        "        relevant_words.extend(extract_relevant_words(user_query, chunk[2]))\n",
        "    relevant_words = list(set(relevant_words))  # Remove duplicates\n",
        "\n",
        "    #### Step 3: Retrieve Table Chunks Using Extracted Words ####\n",
        "    response_tables = supabase.table(\"tables\").select(\"chunk_id, table_data, description, embedding, metadata\").execute()\n",
        "    table_results = []\n",
        "\n",
        "    for record in response_tables.data:\n",
        "        metadata = record.get(\"metadata\", {})\n",
        "        table_data = record.get(\"table_data\", \"\")\n",
        "        table_description = record.get(\"description\", \"\")\n",
        "        table_embedding = record.get(\"embedding\", None)\n",
        "\n",
        "        keyword_match_score = 0\n",
        "        for word in relevant_words:\n",
        "            if re.search(rf\"\\b{re.escape(word)}\\b\", str(metadata), re.IGNORECASE):\n",
        "                keyword_match_score += 0.5\n",
        "            if re.search(rf\"\\b{re.escape(word)}\\b\", table_data, re.IGNORECASE):\n",
        "                keyword_match_score += 0.3\n",
        "            if re.search(rf\"\\b{re.escape(word)}\\b\", table_description, re.IGNORECASE):\n",
        "                keyword_match_score += 0.7\n",
        "\n",
        "        embedding_match_score = 0\n",
        "        if table_embedding:\n",
        "            if isinstance(table_embedding, str):\n",
        "                table_embedding = ast.literal_eval(table_embedding)\n",
        "            table_embedding = np.array(table_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "            if table_embedding.shape == query_embedding.shape:\n",
        "                similarity = 1 - cosine(query_embedding, table_embedding)\n",
        "                embedding_match_score = similarity\n",
        "\n",
        "        final_score = (1.3 * keyword_match_score) + embedding_match_score\n",
        "        if keyword_match_score > 0:  # Only keep tables that match relevant words\n",
        "            table_results.append((record[\"chunk_id\"], \"table\", table_description, final_score))\n",
        "\n",
        "    table_results.sort(key=lambda x: x[3], reverse=True)  # Sort by final score\n",
        "\n",
        "    #### Step 4: Merge & Sort Results ####\n",
        "    all_results = top_text_chunks + table_results[:2]  # Ensure text context is maintained\n",
        "    all_results.sort(key=lambda x: x[3], reverse=True)  # Sort again by relevance\n",
        "\n",
        "    return all_results[:5]  # Return top 5 most relevant results\n"
      ],
      "metadata": {
        "id": "TUX6G_js_eKK"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"Limiti Di Impiego\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7Uw3l8iw_f0",
        "outputId": "9591fd11-6694-4a91-f2a4-0f9d2e5bc8a8"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 5.4732\n",
            "\n",
            "Chunk ID: 05e47d48-c7be-4c38-949e-f599df68a060\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Valvola termostatica che trafila | Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Isolamenti staccati | Anomalia: Perdita d'acqua in riscaldamento ...\n",
            "Relevance: 5.4258\n",
            "\n",
            "Chunk ID: 8843584e-ca30-4846-b7cf-234dfabb403b\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego...\n",
            "Relevance: 0.8592\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.7917\n",
            "\n",
            "Chunk ID: f0acd6ed-9683-4b0a-983a-9b45c9ec18ac\n",
            "Type: text\n",
            "Content: ## 3.1. Sollevamento E Trasporto\n",
            "Durante lo scarico e il posizionamento dell'unità, va posta la massima cura nell'evitare manovre brusche o violente. I trasporti interni dovranno essere eseguiti con cura e delicatamente, evitando di usare come punti di forza i componenti della macchina. ![Image](/co...\n",
            "Relevance: 0.7813\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the operating limits?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY3y8ZNrxRjY",
        "outputId": "d3218cff-e5c0-4672-fd04-81afe9fd79a3"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 05e47d48-c7be-4c38-949e-f599df68a060\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Valvola termostatica che trafila | Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Isolamenti staccati | Anomalia: Perdita d'acqua in riscaldamento ...\n",
            "Relevance: 5.5398\n",
            "\n",
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 5.5133\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.8692\n",
            "\n",
            "Chunk ID: f0acd6ed-9683-4b0a-983a-9b45c9ec18ac\n",
            "Type: text\n",
            "Content: ## 3.1. Sollevamento E Trasporto\n",
            "Durante lo scarico e il posizionamento dell'unità, va posta la massima cura nell'evitare manovre brusche o violente. I trasporti interni dovranno essere eseguiti con cura e delicatamente, evitando di usare come punti di forza i componenti della macchina. ![Image](/co...\n",
            "Relevance: 0.8669\n",
            "\n",
            "Chunk ID: 3fd64fa2-565c-4dea-bb68-cd4e61e8cd41\n",
            "Type: text\n",
            "Content: ## 2.6.2. IRIS Slim Verticale Da Incasso\n",
            "![Image](/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT_artifacts/image_000008_494da229566d9c06790515f5c0b58614e2452ec5bc0bec2b0cd785e86d64fa98.png) *Image Description:* The image depicts a sensor setup with three types of items bein...\n",
            "Relevance: 0.8639\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_relevant_words(query, text):\n",
        "    \"\"\"Finds the most relevant words from the retrieved text based on similarity to the query.\"\"\"\n",
        "    query_words = query.lower().split()\n",
        "    text_words = text.lower().split()\n",
        "    word_counts = Counter(text_words)\n",
        "    relevant_words = [word for word in text_words if word in query_words or word_counts[word] > 1]\n",
        "    return list(set(relevant_words))  # Return unique words\n",
        "\n",
        "def query_requires_table(user_query):\n",
        "    \"\"\"Determines if the query is likely asking for table data.\"\"\"\n",
        "    table_keywords = {\"table\", \"data\", \"values\", \"measurements\", \"limits\", \"thresholds\", \"parameters\"}\n",
        "    return any(word in user_query.lower() for word in table_keywords)\n",
        "\n",
        "def query_supabase(user_query):\n",
        "    \"\"\"Retrieves both text and table chunks based on query, ensuring relevance balance.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "    requires_table = query_requires_table(user_query)\n",
        "\n",
        "    #### Step 1: Retrieve Text Chunks (Vector Search) ####\n",
        "    response_text = supabase.table(\"documents\").select(\"chunk_id, content, embedding, type, metadata\").execute()\n",
        "    text_results = []\n",
        "\n",
        "    for record in response_text.data:\n",
        "        chunk_embedding = record[\"embedding\"]\n",
        "        if isinstance(chunk_embedding, str):\n",
        "            chunk_embedding = ast.literal_eval(chunk_embedding)\n",
        "        chunk_embedding = np.array(chunk_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "        if chunk_embedding.shape == query_embedding.shape:\n",
        "            similarity = 1 - cosine(query_embedding, chunk_embedding)\n",
        "            text_results.append((record[\"chunk_id\"], \"text\", record[\"content\"], similarity))\n",
        "\n",
        "    text_results.sort(key=lambda x: x[3], reverse=True)  # Sort by similarity\n",
        "    top_text_chunks = text_results[:3]  # Keep top 3 text chunks\n",
        "\n",
        "    #### Step 2: Extract Most Relevant Words from Top Text Chunks ####\n",
        "    relevant_words = []\n",
        "    for chunk in top_text_chunks:\n",
        "        relevant_words.extend(extract_relevant_words(user_query, chunk[2]))\n",
        "    relevant_words = list(set(relevant_words))  # Remove duplicates\n",
        "\n",
        "    #### Step 3: Retrieve Table Chunks Using Extracted Words ####\n",
        "    response_tables = supabase.table(\"tables\").select(\"chunk_id, table_data, description, embedding, metadata\").execute()\n",
        "    table_results = []\n",
        "\n",
        "    for record in response_tables.data:\n",
        "        metadata = record.get(\"metadata\", {})\n",
        "        table_data = record.get(\"table_data\", \"\")\n",
        "        table_description = record.get(\"description\", \"\")\n",
        "        table_embedding = record.get(\"embedding\", None)\n",
        "\n",
        "        keyword_match_score = 0\n",
        "        for word in relevant_words:\n",
        "            if re.search(rf\"\\b{re.escape(word)}\\b\", str(metadata), re.IGNORECASE):\n",
        "                keyword_match_score += 0.5\n",
        "            if re.search(rf\"\\b{re.escape(word)}\\b\", table_data, re.IGNORECASE):\n",
        "                keyword_match_score += 0.3\n",
        "            if re.search(rf\"\\b{re.escape(word)}\\b\", table_description, re.IGNORECASE):\n",
        "                keyword_match_score += 0.7\n",
        "\n",
        "        embedding_match_score = 0\n",
        "        if table_embedding:\n",
        "            if isinstance(table_embedding, str):\n",
        "                table_embedding = ast.literal_eval(table_embedding)\n",
        "            table_embedding = np.array(table_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "            if table_embedding.shape == query_embedding.shape:\n",
        "                similarity = 1 - cosine(query_embedding, table_embedding)\n",
        "                embedding_match_score = similarity * 0.5  # Adjusted weight for better balance\n",
        "\n",
        "        final_score = (1.0 * keyword_match_score) + embedding_match_score  # Balanced weight ratio\n",
        "\n",
        "        # Include tables only if they are highly relevant\n",
        "        if (requires_table and keyword_match_score > 0.2) or keyword_match_score > 0.5 or embedding_match_score > 0.3:\n",
        "            table_results.append((record[\"chunk_id\"], \"table\", table_description, final_score))\n",
        "\n",
        "    table_results.sort(key=lambda x: x[3], reverse=True)  # Sort by final score\n",
        "\n",
        "    #### Step 4: Merge & Sort Results ####\n",
        "    final_results = text_results[:3] + table_results[:2]  # Ensure text priority, limit tables\n",
        "    final_results.sort(key=lambda x: x[3], reverse=True)  # Sort again by relevance\n",
        "\n",
        "    return final_results[:5]  # Return top 5 most relevant results\n"
      ],
      "metadata": {
        "id": "Hsr8vFuT7feM"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"usage limit\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyxciCdoH9F0",
        "outputId": "6b4adaa4-f5b4-4b63-e592-4cff877ad212"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 3.9680\n",
            "\n",
            "Chunk ID: e377dbf0-9b8b-4ae7-aab7-eee8f1466ac1\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Il pannello radiante frontale non si  riscalda, Possibili guasti: Presenza di aria nelle tubazioni | Anomalia: La ventilazione non risponde  immediatamente alle nuove  impostazioni, Possibili guasti: La valvola impiega quale minuto ad  aprirsi | Anomalia: ...\n",
            "Relevance: 2.9668\n",
            "\n",
            "Chunk ID: 8843584e-ca30-4846-b7cf-234dfabb403b\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego...\n",
            "Relevance: 0.8015\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.7723\n",
            "\n",
            "Chunk ID: 61b9d9e2-6e2e-4430-ba82-46680cca3884\n",
            "Type: text\n",
            "Content: ## IRIS SLIM / IN\n",
            "Manuale d'installazione ed uso...\n",
            "Relevance: 0.7599\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the usage limit?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5kE0ZdnLpEt",
        "outputId": "118bbb5c-0abc-4379-8322-e855278717a5"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 05e47d48-c7be-4c38-949e-f599df68a060\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Valvola termostatica che trafila | Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Isolamenti staccati | Anomalia: Perdita d'acqua in riscaldamento ...\n",
            "Relevance: 9.1226\n",
            "\n",
            "Chunk ID: e377dbf0-9b8b-4ae7-aab7-eee8f1466ac1\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Il pannello radiante frontale non si  riscalda, Possibili guasti: Presenza di aria nelle tubazioni | Anomalia: La ventilazione non risponde  immediatamente alle nuove  impostazioni, Possibili guasti: La valvola impiega quale minuto ad  aprirsi | Anomalia: ...\n",
            "Relevance: 7.7224\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.8683\n",
            "\n",
            "Chunk ID: 64f46084-3a66-425c-8eb7-86b9ddd27c9c\n",
            "Type: text\n",
            "Content: ## 2.2. Usi Non Previsti E Controindicazioni\n",
            "Non sono ammesse le seguenti applicazioni: - · Funzionamento all'aperto - · Funzionamento in ambienti umidi o esplosivi o polverosi - · Funzionamento in ambienti corrosivi, in particolare per le alette d'alluminio della batteria - · Funzionamento in ambie...\n",
            "Relevance: 0.8667\n",
            "\n",
            "Chunk ID: b7ce279a-5731-4bb6-9f73-274e4e99a2e8\n",
            "Type: text\n",
            "Content: ## 2.1. Uso Previsto\n",
            "Le unità Iris Slim sono progettate per la funzione di riscaldamento, raffrescamento, deumidificazione e filtrazione di ambienti residenziali e terziario (uffici, locali pubblici, o simili)....\n",
            "Relevance: 0.8592\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the operating limits?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRCklKdkGpnH",
        "outputId": "46cc7c16-7db2-4681-d144-4a8ec841512e"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 05e47d48-c7be-4c38-949e-f599df68a060\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Valvola termostatica che trafila | Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Isolamenti staccati | Anomalia: Perdita d'acqua in riscaldamento ...\n",
            "Relevance: 4.0299\n",
            "\n",
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 4.0167\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.8692\n",
            "\n",
            "Chunk ID: f0acd6ed-9683-4b0a-983a-9b45c9ec18ac\n",
            "Type: text\n",
            "Content: ## 3.1. Sollevamento E Trasporto\n",
            "Durante lo scarico e il posizionamento dell'unità, va posta la massima cura nell'evitare manovre brusche o violente. I trasporti interni dovranno essere eseguiti con cura e delicatamente, evitando di usare come punti di forza i componenti della macchina. ![Image](/co...\n",
            "Relevance: 0.8669\n",
            "\n",
            "Chunk ID: 3fd64fa2-565c-4dea-bb68-cd4e61e8cd41\n",
            "Type: text\n",
            "Content: ## 2.6.2. IRIS Slim Verticale Da Incasso\n",
            "![Image](/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT_artifacts/image_000008_494da229566d9c06790515f5c0b58614e2452ec5bc0bec2b0cd785e86d64fa98.png) *Image Description:* The image depicts a sensor setup with three types of items bein...\n",
            "Relevance: 0.8639\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what does it explain about?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDMzAE2jJkhE",
        "outputId": "529590be-edd1-40d0-aa01-dbd7871e77fb"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: 05e47d48-c7be-4c38-949e-f599df68a060\n",
            "Type: table\n",
            "Content: 6.3.1. Troubleshooting IRIS Slim\n",
            "\n",
            "Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Valvola termostatica che trafila | Anomalia: Sono presenti gocce di rugiada sul  pannello frontale, Possibili guasti: Isolamenti staccati | Anomalia: Perdita d'acqua in riscaldamento ...\n",
            "Relevance: 2.6994\n",
            "\n",
            "Chunk ID: 83b3e7ff-bfd9-4290-8267-4a1aee36f4e6\n",
            "Type: table\n",
            "Content: 2.6.1. IRIS Slim Verticale Con Mobile\n",
            "Tabella 2. Dimensioni e peso\n",
            "Grandezza: A (mm), 601: 600...\n",
            "Relevance: 2.5954\n",
            "\n",
            "Chunk ID: ebaa168e-5aa7-45b3-b95f-129870908ee6\n",
            "Type: text\n",
            "Content: ## 1.1.1. Descrizione Dei Simboli\n",
            "![Image](/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT_artifacts/image_000002_c29e9525da2f6b726ee7e068747d7d4a4f7221363d36cb4fac1564d44bbcac1f.png) *Image Description:* The image shows a yellow warning triangle with a black exclamation mar...\n",
            "Relevance: 0.8111\n",
            "\n",
            "Chunk ID: 3fd64fa2-565c-4dea-bb68-cd4e61e8cd41\n",
            "Type: text\n",
            "Content: ## 2.6.2. IRIS Slim Verticale Da Incasso\n",
            "![Image](/content/drive/MyDrive/document_rag_italy/md/Manuale-IRIS_SLIM_IN_TEC_IT_artifacts/image_000008_494da229566d9c06790515f5c0b58614e2452ec5bc0bec2b0cd785e86d64fa98.png) *Image Description:* The image depicts a sensor setup with three types of items bein...\n",
            "Relevance: 0.8103\n",
            "\n",
            "Chunk ID: f0acd6ed-9683-4b0a-983a-9b45c9ec18ac\n",
            "Type: text\n",
            "Content: ## 3.1. Sollevamento E Trasporto\n",
            "Durante lo scarico e il posizionamento dell'unità, va posta la massima cura nell'evitare manovre brusche o violente. I trasporti interni dovranno essere eseguiti con cura e delicatamente, evitando di usare come punti di forza i componenti della macchina. ![Image](/co...\n",
            "Relevance: 0.8065\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# OpenAI API Key\n",
        "OPENAI_API_KEY = \"\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Function to call OpenAI LLM with chat history\n",
        "def call_openai_llm(user_query, retrieved_chunks, chat_history=[]):\n",
        "    \"\"\"Send the query along with retrieved context and chat history to OpenAI API.\"\"\"\n",
        "\n",
        "    # Prepare context from retrieved chunks\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    # Construct messages for conversational memory\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\"},\n",
        "    ]\n",
        "\n",
        "    # Append chat history\n",
        "    messages.extend(chat_history)\n",
        "\n",
        "    # Append current query with retrieved context\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nUser's Question: {user_query}\"})\n",
        "\n",
        "    # Call OpenAI's Chat API with the new format\n",
        "    client = openai.OpenAI(api_key=openai.api_key)  # Ensure you are using the new client-based API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",  # You can change this to another OpenAI model\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content  # Adjusted based on the new API response format\n",
        "\n",
        "    # Append response to chat history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return answer, chat_history"
      ],
      "metadata": {
        "id": "sOMrRXrqGnir"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"is there any statistical or tables data?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znbbdZICJyAF",
        "outputId": "49eef86c-47d6-410e-c87d-2ac7ee277a52"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " In the provided chunks, there is reference to a table in Chunk 5: \"Tabella 3. Dimensioni e peso\" which indicates dimensions and weight. However, the specific content or data of this table is not provided in the text snippets you shared. Therefore, while it is mentioned that such statistical or table data exists, the actual data from the table is not available in the excerpts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what does it explain about?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S5imWMnJz9N",
        "outputId": "f9a41998-b56b-4000-e371-feff84bf6e70"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The provided information discusses various aspects of the IRIS Slim appliance, including troubleshooting common issues, dimensions and weight for specific models, and handling instructions during transportation. Here's a breakdown of what each chunk explains:\n",
            "\n",
            "1. **Chunk 1 (Troubleshooting IRIS Slim)**: This section lists potential anomalies and their possible causes associated with the IRIS Slim. It covers issues like condensation on the front panel, water leakage in both heating and cooling modes, noisy operations due to fan damages or misalignments, and other specific conditions leading to equipment malfunctions.\n",
            "\n",
            "2. **Chunk 2 (IRIS Slim Verticale Con Mobile)**: Provides the dimensions and weight for a particular model of the IRIS Slim, specifically stating that model 601 has a height of 600 mm.\n",
            "\n",
            "3. **Chunk 3 (Description of Symbols)**: Describes a symbol used in the manual or on the appliance, which is a yellow warning triangle with a black exclamation mark, typically indicating a general warning or caution.\n",
            "\n",
            "4. **Chunk 4 (IRIS Slim Verticale Da Incasso)**: Describes an image related to a sensor setup used for calibration based on measurements of weight, which might be part of the internal components or functionality checks of the IRIS Slim.\n",
            "\n",
            "5. **Chunk 5 (Lifting and Transporting)**: Provides guidelines on how to properly handle the unit during transportation and placement, emphasizing the need to avoid rough or forceful maneuvers and to carefully move the unit without using its components as leverage points. An associated warning sign image reiterates the need for caution.\n",
            "\n",
            "Overall, the text explains the operation, maintenance, and handling of the IRIS Slim appliance, aiming to ensure safe usage and troubleshooting of common operational issues.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"What are the operating limits of the IRIS Slim unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6655d1-5ec2-444f-c1f2-378986ec5dc0",
        "id": "aedereIRKZDf"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The operating limits of the IRIS Slim unit, specifically related to its dimensions and weight, as per the provided chunks are as follows:\n",
            "\n",
            "- **Dimensions:** The device has a height of 601 mm, and the total length ranges from 163 to 198 mm, with a height from 96 to 129 mm. \n",
            "- **Weight:** The unit's weight varies between 17 kg to 26 kg.\n",
            "\n",
            "These specifications indicate the physical constraints within which the IRIS Slim unit operates. However, any operational temperature ranges, pressure limits, or electrical specifications were not provided in the retrieved information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the Limits of use of the unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcq7SFrNKitb",
        "outputId": "2844dd3c-c6cd-40ba-86dd-bde488b459d8"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The limits of use for the unit are specified in Chunk 3 and Chunk 5:\n",
            "\n",
            "1. **Non-Permitted Applications and Contraindications (Chunk 3)**\n",
            "   - The unit should not be operated outdoors.\n",
            "   - It should not be used in humid, explosive, or dusty environments.\n",
            "   - It is not suitable for operation in corrosive environments, especially concerning the aluminum fins of the battery.\n",
            "   - The unit should not be operated in areas subjected to electromagnetic disturbances.\n",
            "   - It is not intended for use by individuals (including children) with reduced physical, mental, or sensory capacities, or by those who have not received adequate instruction, unless they are under the supervision of a person responsible for their safety.\n",
            "\n",
            "2. **Employment Limits (Chunk 5)**\n",
            "   - It is advised to operate the machine at the extremes of the specified employment limits only for short periods, as long-term operation may reduce the normal lifespan of the components.\n",
            "\n",
            "These limitations are designed to ensure safe and efficient operation of the unit while minimizing the risk of damage or failure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the usage limit of the unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgTN5Z-_K6bb",
        "outputId": "5a36d34a-d42e-48e3-f2e1-dc270fbdb713"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The usage limits of the unit, as described in Chunk 4 (\"2.5. Limiti Di Impiego\"), recommend operating the machine at the extremes of its operational limits only for short periods. This is because prolonged operation can reduce the normal lifespan of its components. This guidance suggests that while the unit can handle extreme conditions, it is best used within its normal operating parameters to ensure longevity and optimal performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wPo7BgfLLqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}