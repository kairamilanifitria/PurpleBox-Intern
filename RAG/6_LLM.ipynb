{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "104kL32YRzVbz1mx0KYU5qvoe-dwprwer",
      "authorship_tag": "ABX9TyOni4T55YtsP333DDje8IUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/PurpleBox-Intern/blob/main/RAG/6_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRIAL** : Using GROQ API key. I will update the code with OpenAI API as soon as possible."
      ],
      "metadata": {
        "id": "TsVTyY66VVRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "id": "9fyj7SOMUDBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Your Groq API key\n",
        "GROQ_API_KEY = \"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\"\n",
        "\n",
        "\n",
        "def call_groq_llm(user_query, retrieved_chunks):\n",
        "    \"\"\"Send the query along with retrieved context to Groq API and return the response.\"\"\"\n",
        "\n",
        "    # Print retrieved chunks for debugging\n",
        "    print(\"\\nüîπ Retrieved Chunks:\")\n",
        "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "        print(f\"Chunk {i} (ID: {chunk[0]}, Type: {chunk[1]}):\\n{chunk[2][:500]}...\\nRelevance: {chunk[3]:.4f}\\n\")\n",
        "\n",
        "    # Prepare context for LLM\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    prompt = f\"\"\"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\n",
        "\n",
        "    Context:\n",
        "    {context_text}\n",
        "\n",
        "    User's Question: {user_query}\n",
        "\n",
        "    Provide a clear and concise response.\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"qwen-qwq-32b\",  # Adjust this based on your Groq model selection\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "        print(\"\\nüîπ Chatbot Response:\\n\", answer)\n",
        "        return answer\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Error:\", response.text)\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "rJEt6MQBUDFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"_______________\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "if retrieved_chunks:\n",
        "    call_groq_llm(user_query, retrieved_chunks)\n",
        "else:\n",
        "    print(\"No relevant information found.\")\n"
      ],
      "metadata": {
        "id": "bqmVoCoyUPEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to fully testing, try to run the `4_EMBEDDING.ipynb` , then `5_RETRIEVAL.ipyb` and `6_LLM.ipynb` in one notebook."
      ],
      "metadata": {
        "id": "9J7JqcfKUmxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "copy those cells and run it in sequence (*because some function are connect between each other module, so it can't show result if only run in this notebook only*)"
      ],
      "metadata": {
        "id": "VZwk-qq9U60i"
      }
    }
  ]
}