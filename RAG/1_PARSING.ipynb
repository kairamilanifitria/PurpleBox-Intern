{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "104kL32YRzVbz1mx0KYU5qvoe-dwprwer",
      "authorship_tag": "ABX9TyOFC7JZkE1yNE29YiCTbFrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/PurpleBox-Intern/blob/main/RAG/1_PARSING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import logging\n",
        "import time\n",
        "import warnings\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Install dependencies\n",
        "subprocess.run([\n",
        "    \"pip\", \"install\",\n",
        "    \"llama-index>=0.12.8\", \"llama-index-core>=0.12.8\",\n",
        "    \"llama-index-node-parser-docling>=0.3.0\", \"llama-index-readers-docling>=0.3.0\",\n",
        "    \"pypdf2>=3.0.1\", \"easyocr>=1.7.2\"\n",
        "], check=True)\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"CUDA GPU is enabled: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"MPS GPU is enabled.\")\n",
        "else:\n",
        "    raise EnvironmentError(\"No GPU or MPS device found.\")\n",
        "\n",
        "# Import installed libraries after installation\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling_core.types.doc import ImageRefMode\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption, WordFormatOption\n",
        "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.pipeline.simple_pipeline import SimplePipeline\n",
        "from docling.datamodel.settings import settings\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "_log = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "7S4SxzM_I6lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_RESOLUTION_SCALE = 2.0\n",
        "\n",
        "def create_pipeline_options(input_format):\n",
        "    \"\"\"Creates dynamic pipeline options based on the input format.\"\"\"\n",
        "    if input_format == InputFormat.PDF:\n",
        "        return PdfFormatOption(\n",
        "            pipeline_options=PdfPipelineOptions(\n",
        "                do_table_structure=True,\n",
        "                generate_page_images=True,\n",
        "                generate_picture_images=True,\n",
        "                images_scale=IMAGE_RESOLUTION_SCALE,\n",
        "            )\n",
        "        )\n",
        "    elif input_format == InputFormat.DOCX:\n",
        "        return WordFormatOption(pipeline_cls=SimplePipeline)\n",
        "    return None  # Other formats not supported\n",
        "\n",
        "def initialize_converter():\n",
        "    \"\"\"Initializes the document converter with multiformat support.\"\"\"\n",
        "    allowed_formats = [InputFormat.PDF, InputFormat.DOCX]\n",
        "    format_options = {fmt: create_pipeline_options(fmt) for fmt in allowed_formats if create_pipeline_options(fmt)}\n",
        "    return DocumentConverter(allowed_formats=allowed_formats, format_options=format_options)\n",
        "\n",
        "def convert_and_save(input_paths, output_dir, image_mode=ImageRefMode.REFERENCED):\n",
        "    \"\"\"Converts documents to Markdown and saves the output.\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    doc_converter = initialize_converter()\n",
        "    conv_results = doc_converter.convert_all(input_paths)\n",
        "\n",
        "    for res in conv_results:\n",
        "        file_name = res.input.file.stem\n",
        "        markdown_path = output_dir / f\"{file_name}.md\"\n",
        "        res.document.save_as_markdown(markdown_path, image_mode=image_mode)\n",
        "        _log.info(f\"Markdown content saved to {markdown_path}\")\n",
        "\n",
        "def extract_all_nodes_with_image_refs(md_file_path, output_dir):\n",
        "    \"\"\"Extracts all nodes from a markdown file, including image references.\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    output_path = output_dir / f\"{md_file_path.stem}_nodes.json\"\n",
        "\n",
        "    try:\n",
        "        with open(md_file_path, 'r', encoding='utf-8') as f:\n",
        "            markdown_content = f.read()\n",
        "    except (UnicodeDecodeError, FileNotFoundError):\n",
        "        print(f\"Error: Could not read file {md_file_path}\")\n",
        "        return\n",
        "\n",
        "    all_nodes, current_text_block = [], \"\"\n",
        "    for line in markdown_content.split('\\n'):\n",
        "        if '![' in line and '(' in line and ')' in line:\n",
        "            parts = line.split('(')\n",
        "            image_path = parts[1].split(')')[0] if len(parts) > 1 else None\n",
        "            node_text = parts[0].split('[')[1].split(']')[0] if '[' in parts[0] else \"\"\n",
        "\n",
        "            if current_text_block.strip():\n",
        "                all_nodes.append({\"index\": len(all_nodes) + 1, \"text\": current_text_block.strip(), \"image_path\": None})\n",
        "            all_nodes.append({\"index\": len(all_nodes) + 1, \"text\": node_text, \"image_path\": image_path})\n",
        "            current_text_block = \"\"\n",
        "        else:\n",
        "            current_text_block += line + \"\\n\"\n",
        "\n",
        "    if current_text_block.strip():\n",
        "        all_nodes.append({\"index\": len(all_nodes) + 1, \"text\": current_text_block.strip(), \"image_path\": None})\n",
        "\n",
        "    with output_path.open(\"w\") as fp:\n",
        "        json.dump({\"file_name\": md_file_path.name, \"nodes\": all_nodes}, fp, indent=4)\n",
        "    print(f\"Extracted {len(all_nodes)} nodes from {md_file_path.name} to {output_path}\")\n"
      ],
      "metadata": {
        "id": "DaXkuz0AI-40"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    settings.debug.profile_pipeline_timings = True\n",
        "    input_paths = [Path(\"_____\")]\n",
        "    output_dir = \"______\"\n",
        "    convert_and_save(input_paths, output_dir)\n",
        "\n",
        "    for md_file in Path(output_dir).glob(\"*.md\"):\n",
        "        extract_all_nodes_with_image_refs(md_file, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "LStp-OPIJFYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gZhN3PviJpOQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}