{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1XLPzbzeza9eXklu3hYBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/PurpleBox-Intern/blob/main/RAG/7_RETRIEVAL-LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RETRIEVAL-LLM"
      ],
      "metadata": {
        "id": "OuT9m9Byy-3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase numpy psycopg2"
      ],
      "metadata": {
        "id": "ygLQrkHauqUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import uuid\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize Supabase\n",
        "SUPABASE_URL = \"\"\n",
        "SUPABASE_KEY = \"\"\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Load Embedding Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ],
      "metadata": {
        "id": "HtTo1NiIwTuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieval"
      ],
      "metadata": {
        "id": "bjRtvYtnzS1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_keywords_simple(text):\n",
        "    \"\"\"Extracts important words from a query using simple filtering.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def query_requires_table(user_query):\n",
        "    \"\"\"Determines if the query is likely asking for table data.\"\"\"\n",
        "    table_keywords = {\"table\", \"data\", \"values\", \"measurements\", \"limits\", \"thresholds\", \"parameters\", \"average\", \"sum\", \"percentage\"}\n",
        "    return any(word in user_query.lower() for word in table_keywords)\n",
        "\n",
        "def get_most_similar_keywords(query_keywords, top_text_chunks):\n",
        "    \"\"\"Extracts most relevant words from top retrieved text chunks.\"\"\"\n",
        "    all_text_words = set()\n",
        "    for chunk in top_text_chunks:\n",
        "        chunk_words = set(word_tokenize(chunk[2].lower()))  # Extract words from chunk text\n",
        "        all_text_words.update(chunk_words)\n",
        "    common_words = [word for word in query_keywords if word in all_text_words]\n",
        "    return common_words if common_words else query_keywords  # Fallback to original keywords if no match\n",
        "\n",
        "def query_supabase(user_query):\n",
        "    \"\"\"Retrieves both text and table chunks based on query, ensuring relevance balance.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "    keywords = extract_keywords_simple(user_query)\n",
        "    requires_table = query_requires_table(user_query)\n",
        "\n",
        "    #### Step 1: Retrieve Text Chunks (Vector Search) ####\n",
        "    response_text = supabase.table(\"documents\").select(\"chunk_id, content, embedding, type, metadata\").execute()\n",
        "    text_results = []\n",
        "\n",
        "    for record in response_text.data:\n",
        "        chunk_embedding = ast.literal_eval(record[\"embedding\"]) if isinstance(record[\"embedding\"], str) else record[\"embedding\"]\n",
        "        chunk_embedding = np.array(chunk_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "        if chunk_embedding.shape == query_embedding.shape:\n",
        "            similarity = 1 - cosine(query_embedding, chunk_embedding)\n",
        "            text_results.append((record[\"chunk_id\"], \"text\", record[\"content\"], similarity))\n",
        "\n",
        "    text_results.sort(key=lambda x: x[3], reverse=True)\n",
        "    top_text_chunks = text_results[:3]\n",
        "\n",
        "    #### Step 2: Expand Query Using Retrieved Text ####\n",
        "    refined_keywords = get_most_similar_keywords(keywords, top_text_chunks)\n",
        "\n",
        "    #### Step 3: Retrieve Table Chunks Using Specialized Scoring ####\n",
        "    response_tables = supabase.table(\"tables\").select(\"chunk_id, table_data, description, embedding, metadata\").execute()\n",
        "    table_results = []\n",
        "    table_weight = 2.5 if requires_table else 1.5  # Increase weight dynamically\n",
        "\n",
        "    for record in response_tables.data:\n",
        "        table_embedding = ast.literal_eval(record[\"embedding\"]) if isinstance(record[\"embedding\"], str) else record[\"embedding\"]\n",
        "        table_embedding = np.array(table_embedding, dtype=np.float32).flatten()\n",
        "        table_data = record[\"table_data\"].lower()\n",
        "        table_description = record[\"description\"].lower()\n",
        "        keyword_match_score = sum(3 if word in table_data.split(\" \")[:5] else 1 for word in refined_keywords if word in table_data or word in table_description)\n",
        "\n",
        "        if table_embedding.shape == query_embedding.shape:\n",
        "            embedding_similarity = 1 - cosine(query_embedding, table_embedding)\n",
        "            keyword_embedding_score = sum(1 - cosine(get_embedding(word), table_embedding) for word in refined_keywords) / max(len(refined_keywords), 1)\n",
        "\n",
        "            final_table_score = (embedding_similarity ** 0.8) * 0.2 + (keyword_match_score ** 2.5) * 0.6 + (keyword_embedding_score ** 1.2) * 0.2\n",
        "\n",
        "            if final_table_score > 0:\n",
        "                table_results.append((record[\"chunk_id\"], \"table\", record[\"description\"], final_table_score))\n",
        "\n",
        "    table_results.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "    #### Step 4: Merge & Rank Results with Adaptive Prioritization ####\n",
        "    if table_results and table_results[0][3] > 0.75:\n",
        "        final_results = [table_results[0]] + text_results[:2] + table_results[1:2] + text_results[2:]\n",
        "    else:\n",
        "        final_results = text_results[:3] + table_results[:2]  # Natural sorting if no table is required\n",
        "\n",
        "    return final_results[:5]  # Return top 5 most relevant results"
      ],
      "metadata": {
        "id": "xYeEZR9cNT3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm function"
      ],
      "metadata": {
        "id": "_kl2suhBzOnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# OpenAI API Key\n",
        "OPENAI_API_KEY = \"\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Function to call OpenAI LLM with chat history\n",
        "def call_openai_llm(user_query, retrieved_chunks, chat_history=[]):\n",
        "    \"\"\"Send the query along with retrieved context and chat history to OpenAI API.\"\"\"\n",
        "\n",
        "    # Prepare context from retrieved chunks\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    # Construct messages for conversational memory\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\"},\n",
        "    ]\n",
        "\n",
        "    # Append chat history\n",
        "    messages.extend(chat_history)\n",
        "\n",
        "    # Append current query with retrieved context\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nUser's Question: {user_query}\"})\n",
        "\n",
        "    # Call OpenAI's Chat API with the new format\n",
        "    client = openai.OpenAI(api_key=openai.api_key)  # Ensure you are using the new client-based API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",  # You can change this to another OpenAI model\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content  # Adjusted based on the new API response format\n",
        "\n",
        "    # Append response to chat history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return answer, chat_history"
      ],
      "metadata": {
        "id": "mDMh71-mrP_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "-KxZMPImzXtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieving chunks only"
      ],
      "metadata": {
        "id": "e3ZX6dlkzkBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"usage limit\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df14738-0107-401f-a5fa-1b154b69ef34",
        "id": "4qC7O9VPVNNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 0.8898\n",
            "\n",
            "Chunk ID: 8843584e-ca30-4846-b7cf-234dfabb403b\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego...\n",
            "Relevance: 0.8015\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.7723\n",
            "\n",
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 0.8775\n",
            "\n",
            "Chunk ID: 61b9d9e2-6e2e-4430-ba82-46680cca3884\n",
            "Type: text\n",
            "Content: ## IRIS SLIM / IN\n",
            "Manuale d'installazione ed uso...\n",
            "Relevance: 0.7599\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the usage limit?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c045c0-40d5-40c8-ee56-26ed1159145f",
        "id": "3vmGoHjFVNNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 °C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 °C | Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 0.9077\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perché il funzionamento per lunghi periodi può ridurre la normale durata dei componenti....\n",
            "Relevance: 0.8683\n",
            "\n",
            "Chunk ID: 64f46084-3a66-425c-8eb7-86b9ddd27c9c\n",
            "Type: text\n",
            "Content: ## 2.2. Usi Non Previsti E Controindicazioni\n",
            "Non sono ammesse le seguenti applicazioni: - · Funzionamento all'aperto - · Funzionamento in ambienti umidi o esplosivi o polverosi - · Funzionamento in ambienti corrosivi, in particolare per le alette d'alluminio della batteria - · Funzionamento in ambie...\n",
            "Relevance: 0.8667\n",
            "\n",
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 0.8943\n",
            "\n",
            "Chunk ID: b7ce279a-5731-4bb6-9f73-274e4e99a2e8\n",
            "Type: text\n",
            "Content: ## 2.1. Uso Previsto\n",
            "Le unità Iris Slim sono progettate per la funzione di riscaldamento, raffrescamento, deumidificazione e filtrazione di ambienti residenziali e terziario (uffici, locali pubblici, o simili)....\n",
            "Relevance: 0.8592\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add the llm response"
      ],
      "metadata": {
        "id": "nnKHePj6znbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the usage limit of the unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\n🔹 Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHKoJBqWcknf",
        "outputId": "c737c10c-1cce-4f65-cf4d-f9a0b37c246c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Chatbot Response:\n",
            " The usage limits of the unit, as outlined in Chunk 1 under \"2.5. Limiti Di Impiego,\" include the following specifications:\n",
            "\n",
            "- **Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz**: 5 - 70 °C\n",
            "- **Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz**: 10 - 35 °C\n",
            "- **Alimentazione elettrica: Umidità relativa ripresa aria, 220 - 240 V / 50 Hz**: 10 - 70 %\n"
          ]
        }
      ]
    }
  ]
}