{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1XLPzbzeza9eXklu3hYBk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/PurpleBox-Intern/blob/main/RAG/7_RETRIEVAL-LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RETRIEVAL-LLM"
      ],
      "metadata": {
        "id": "OuT9m9Byy-3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase numpy psycopg2"
      ],
      "metadata": {
        "id": "ygLQrkHauqUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import uuid\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize Supabase\n",
        "SUPABASE_URL = \"\"\n",
        "SUPABASE_KEY = \"\"\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Load Embedding Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ],
      "metadata": {
        "id": "HtTo1NiIwTuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieval"
      ],
      "metadata": {
        "id": "bjRtvYtnzS1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_keywords_simple(text):\n",
        "    \"\"\"Extracts important words from a query using simple filtering.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def query_requires_table(user_query):\n",
        "    \"\"\"Determines if the query is likely asking for table data.\"\"\"\n",
        "    table_keywords = {\"table\", \"data\", \"values\", \"measurements\", \"limits\", \"thresholds\", \"parameters\", \"average\", \"sum\", \"percentage\"}\n",
        "    return any(word in user_query.lower() for word in table_keywords)\n",
        "\n",
        "def get_most_similar_keywords(query_keywords, top_text_chunks):\n",
        "    \"\"\"Extracts most relevant words from top retrieved text chunks.\"\"\"\n",
        "    all_text_words = set()\n",
        "    for chunk in top_text_chunks:\n",
        "        chunk_words = set(word_tokenize(chunk[2].lower()))  # Extract words from chunk text\n",
        "        all_text_words.update(chunk_words)\n",
        "    common_words = [word for word in query_keywords if word in all_text_words]\n",
        "    return common_words if common_words else query_keywords  # Fallback to original keywords if no match\n",
        "\n",
        "def query_supabase(user_query):\n",
        "    \"\"\"Retrieves both text and table chunks based on query, ensuring relevance balance.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "    keywords = extract_keywords_simple(user_query)\n",
        "    requires_table = query_requires_table(user_query)\n",
        "\n",
        "    #### Step 1: Retrieve Text Chunks (Vector Search) ####\n",
        "    response_text = supabase.table(\"documents\").select(\"chunk_id, content, embedding, type, metadata\").execute()\n",
        "    text_results = []\n",
        "\n",
        "    for record in response_text.data:\n",
        "        chunk_embedding = ast.literal_eval(record[\"embedding\"]) if isinstance(record[\"embedding\"], str) else record[\"embedding\"]\n",
        "        chunk_embedding = np.array(chunk_embedding, dtype=np.float32).flatten()\n",
        "\n",
        "        if chunk_embedding.shape == query_embedding.shape:\n",
        "            similarity = 1 - cosine(query_embedding, chunk_embedding)\n",
        "            text_results.append((record[\"chunk_id\"], \"text\", record[\"content\"], similarity))\n",
        "\n",
        "    text_results.sort(key=lambda x: x[3], reverse=True)\n",
        "    top_text_chunks = text_results[:3]\n",
        "\n",
        "    #### Step 2: Expand Query Using Retrieved Text ####\n",
        "    refined_keywords = get_most_similar_keywords(keywords, top_text_chunks)\n",
        "\n",
        "    #### Step 3: Retrieve Table Chunks Using Specialized Scoring ####\n",
        "    response_tables = supabase.table(\"tables\").select(\"chunk_id, table_data, description, embedding, metadata\").execute()\n",
        "    table_results = []\n",
        "    table_weight = 2.5 if requires_table else 1.5  # Increase weight dynamically\n",
        "\n",
        "    for record in response_tables.data:\n",
        "        table_embedding = ast.literal_eval(record[\"embedding\"]) if isinstance(record[\"embedding\"], str) else record[\"embedding\"]\n",
        "        table_embedding = np.array(table_embedding, dtype=np.float32).flatten()\n",
        "        table_data = record[\"table_data\"].lower()\n",
        "        table_description = record[\"description\"].lower()\n",
        "        keyword_match_score = sum(3 if word in table_data.split(\" \")[:5] else 1 for word in refined_keywords if word in table_data or word in table_description)\n",
        "\n",
        "        if table_embedding.shape == query_embedding.shape:\n",
        "            embedding_similarity = 1 - cosine(query_embedding, table_embedding)\n",
        "            keyword_embedding_score = sum(1 - cosine(get_embedding(word), table_embedding) for word in refined_keywords) / max(len(refined_keywords), 1)\n",
        "\n",
        "            final_table_score = (embedding_similarity ** 0.8) * 0.2 + (keyword_match_score ** 2.5) * 0.6 + (keyword_embedding_score ** 1.2) * 0.2\n",
        "\n",
        "            if final_table_score > 0:\n",
        "                table_results.append((record[\"chunk_id\"], \"table\", record[\"description\"], final_table_score))\n",
        "\n",
        "    table_results.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "    #### Step 4: Merge & Rank Results with Adaptive Prioritization ####\n",
        "    if table_results and table_results[0][3] > 0.75:\n",
        "        final_results = [table_results[0]] + text_results[:2] + table_results[1:2] + text_results[2:]\n",
        "    else:\n",
        "        final_results = text_results[:3] + table_results[:2]  # Natural sorting if no table is required\n",
        "\n",
        "    return final_results[:5]  # Return top 5 most relevant results"
      ],
      "metadata": {
        "id": "xYeEZR9cNT3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm function"
      ],
      "metadata": {
        "id": "_kl2suhBzOnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# OpenAI API Key\n",
        "OPENAI_API_KEY = \"\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Function to call OpenAI LLM with chat history\n",
        "def call_openai_llm(user_query, retrieved_chunks, chat_history=[]):\n",
        "    \"\"\"Send the query along with retrieved context and chat history to OpenAI API.\"\"\"\n",
        "\n",
        "    # Prepare context from retrieved chunks\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    # Construct messages for conversational memory\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\"},\n",
        "    ]\n",
        "\n",
        "    # Append chat history\n",
        "    messages.extend(chat_history)\n",
        "\n",
        "    # Append current query with retrieved context\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nUser's Question: {user_query}\"})\n",
        "\n",
        "    # Call OpenAI's Chat API with the new format\n",
        "    client = openai.OpenAI(api_key=openai.api_key)  # Ensure you are using the new client-based API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",  # You can change this to another OpenAI model\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content  # Adjusted based on the new API response format\n",
        "\n",
        "    # Append response to chat history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return answer, chat_history"
      ],
      "metadata": {
        "id": "mDMh71-mrP_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "-KxZMPImzXtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieving chunks only"
      ],
      "metadata": {
        "id": "e3ZX6dlkzkBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"usage limit\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df14738-0107-401f-a5fa-1b154b69ef34",
        "id": "4qC7O9VPVNNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 Â°C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 Â°C | Alimentazione elettrica: UmiditÃ  relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 0.8898\n",
            "\n",
            "Chunk ID: 8843584e-ca30-4846-b7cf-234dfabb403b\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego...\n",
            "Relevance: 0.8015\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perchÃ© il funzionamento per lunghi periodi puÃ² ridurre la normale durata dei componenti....\n",
            "Relevance: 0.7723\n",
            "\n",
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 0.8775\n",
            "\n",
            "Chunk ID: 61b9d9e2-6e2e-4430-ba82-46680cca3884\n",
            "Type: text\n",
            "Content: ## IRIS SLIM / IN\n",
            "Manuale d'installazione ed uso...\n",
            "Relevance: 0.7599\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the usage limit?\"\n",
        "\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "for chunk in retrieved_chunks:\n",
        "    print(f\"Chunk ID: {chunk[0]}\\nType: {chunk[1]}\\nContent: {chunk[2][:300]}...\\nRelevance: {chunk[3]:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c045c0-40d5-40c8-ee56-26ed1159145f",
        "id": "3vmGoHjFVNNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk ID: a551dee1-3d8e-485d-b25e-769ee09d5b20\n",
            "Type: table\n",
            "Content: 2.5. Limiti Di Impiego\n",
            "Tabella 1. Limiti di impiego\n",
            "Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz: 5 - 70 Â°C | Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz: 10 - 35 Â°C | Alimentazione elettrica: UmiditÃ  relativa ripresa aria, 220 - 240 V ...\n",
            "Relevance: 0.9077\n",
            "\n",
            "Chunk ID: 76d151bc-6a7a-442e-b924-981cc588e5b2\n",
            "Type: text\n",
            "Content: ## 2.5. Limiti Di Impiego\n",
            "Si consiglia di far lavorare la macchina agli estremi dei suddetti limiti di impiego solo per brevi periodi, perchÃ© il funzionamento per lunghi periodi puÃ² ridurre la normale durata dei componenti....\n",
            "Relevance: 0.8683\n",
            "\n",
            "Chunk ID: 64f46084-3a66-425c-8eb7-86b9ddd27c9c\n",
            "Type: text\n",
            "Content: ## 2.2. Usi Non Previsti E Controindicazioni\n",
            "Non sono ammesse le seguenti applicazioni: - Â· Funzionamento all'aperto - Â· Funzionamento in ambienti umidi o esplosivi o polverosi - Â· Funzionamento in ambienti corrosivi, in particolare per le alette d'alluminio della batteria - Â· Funzionamento in ambie...\n",
            "Relevance: 0.8667\n",
            "\n",
            "Chunk ID: 8941c9c0-9dc7-44c4-bf17-a05a40e824ff\n",
            "Type: table\n",
            "Content: Indice\n",
            "\n",
            "...\n",
            "Relevance: 0.8943\n",
            "\n",
            "Chunk ID: b7ce279a-5731-4bb6-9f73-274e4e99a2e8\n",
            "Type: text\n",
            "Content: ## 2.1. Uso Previsto\n",
            "Le unitÃ  Iris Slim sono progettate per la funzione di riscaldamento, raffrescamento, deumidificazione e filtrazione di ambienti residenziali e terziario (uffici, locali pubblici, o simili)....\n",
            "Relevance: 0.8592\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add the llm response"
      ],
      "metadata": {
        "id": "nnKHePj6znbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "user_query = \"what are the usage limit of the unit?\"\n",
        "retrieved_chunks = query_supabase(user_query)\n",
        "chat_history = []  # Store conversation history\n",
        "\n",
        "if retrieved_chunks:\n",
        "    response, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "    print(\"\\nðŸ”¹ Chatbot Response:\\n\", response)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHKoJBqWcknf",
        "outputId": "c737c10c-1cce-4f65-cf4d-f9a0b37c246c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ Chatbot Response:\n",
            " The usage limits of the unit, as outlined in Chunk 1 under \"2.5. Limiti Di Impiego,\" include the following specifications:\n",
            "\n",
            "- **Alimentazione elettrica: Temperatura acqua ingresso batteria, 220 - 240 V / 50 Hz**: 5 - 70 Â°C\n",
            "- **Alimentazione elettrica: Temperatura ripresa aria, 220 - 240 V / 50 Hz**: 10 - 35 Â°C\n",
            "- **Alimentazione elettrica: UmiditÃ  relativa ripresa aria, 220 - 240 V / 50 Hz**: 10 - 70 %\n"
          ]
        }
      ]
    }
  ]
}