{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "104kL32YRzVbz1mx0KYU5qvoe-dwprwer",
      "authorship_tag": "ABX9TyPn/u2KsJy8mFaP6A7UYP7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/PurpleBox-Intern/blob/main/RAG/2_IMAGE_DESCRIPTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms.functional import InterpolationMode"
      ],
      "metadata": {
        "id": "gZhN3PviJpOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_internvl_model():\n",
        "    path = 'OpenGVLab/InternVL2_5-1B'\n",
        "    model = AutoModel.from_pretrained(\n",
        "        path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        use_flash_attn=True,\n",
        "        trust_remote_code=True\n",
        "    ).eval().cuda()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
        "    return model, tokenizer\n",
        "\n",
        "def build_transform(input_size=448):\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "def load_image(image_file, input_size=448):\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "    transform = build_transform(input_size)\n",
        "    pixel_values = transform(image).unsqueeze(0).to(torch.bfloat16).cuda()\n",
        "    return pixel_values\n",
        "\n",
        "def extract_images_and_context(markdown_path):\n",
        "    with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    image_data = []\n",
        "    for i, line in enumerate(lines):\n",
        "        match = re.search(r'!\\[.*?\\]\\((.*?)\\)', line)\n",
        "        if match:\n",
        "            img_path = match.group(1)\n",
        "            context_before = \" \".join(lines[max(0, i-2):i]).strip()\n",
        "            context_after = \" \".join(lines[i+1:min(len(lines), i+3)]).strip()\n",
        "            image_data.append((img_path, context_before, context_after))\n",
        "    return image_data, lines\n",
        "\n",
        "def generate_caption(model, tokenizer, image_path, context_before, context_after):\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Warning: Image not found - {image_path}\")\n",
        "        return \"[Image description unavailable]\"\n",
        "\n",
        "    pixel_values = load_image(image_path)\n",
        "    prompt = f\"<image>\\nContext: {context_before} ... {context_after}. Please describe the image shortly.\"\n",
        "    generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
        "    response = model.chat(tokenizer, pixel_values, prompt, generation_config)\n",
        "    return response\n",
        "\n",
        "def update_markdown(markdown_path, image_data, lines):\n",
        "    new_lines = []\n",
        "    for line in lines:\n",
        "        new_lines.append(line)\n",
        "        match = re.search(r'!\\[.*?\\]\\((.*?)\\)', line)\n",
        "        if match:\n",
        "            img_path = match.group(1)\n",
        "            caption = next((desc for img, _, _, desc in image_data if img == img_path), \"[Image description unavailable]\")\n",
        "            new_lines.append(f\"\\n*Image Description:* {caption}\\n\")\n",
        "    with open(markdown_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(new_lines)\n",
        "\n",
        "def main(markdown_path, image_folder):\n",
        "    model, tokenizer = load_internvl_model()\n",
        "    image_data, lines = extract_images_and_context(markdown_path)\n",
        "    enriched_data = []\n",
        "    for img_path, context_before, context_after in image_data:\n",
        "        full_image_path = os.path.join(image_folder, img_path)\n",
        "        caption = generate_caption(model, tokenizer, full_image_path, context_before, context_after)\n",
        "        enriched_data.append((img_path, context_before, context_after, caption))\n",
        "    update_markdown(markdown_path, enriched_data, lines)\n",
        "    print(\"Markdown updated with image descriptions!\")"
      ],
      "metadata": {
        "id": "92egIys0NSMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage md, IMAGE folder\n",
        "main(\"_________.md\", \"{filename}_artifacts\")"
      ],
      "metadata": {
        "id": "HABEdAmeNhpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}