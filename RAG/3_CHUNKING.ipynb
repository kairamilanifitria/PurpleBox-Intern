{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "104kL32YRzVbz1mx0KYU5qvoe-dwprwer",
      "authorship_tag": "ABX9TyOz4rwLoHed9KRUTM6wg5Sl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/PurpleBox-Intern/blob/main/RAG/3_CHUNKING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load Markdown file\n",
        "file_path = \"_________.md\"\n",
        "file_name = os.path.basename(file_path)\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    markdown_text = file.read()\n",
        "\n",
        "# Function to check if a chunk contains a Markdown table\n",
        "def is_table(chunk):\n",
        "    return bool(re.search(r'^\\|.*\\|\\n\\|[-| ]+\\|\\n(\\|.*\\|\\n)*', chunk, re.MULTILINE))\n",
        "\n",
        "# Function to extract and split long tables\n",
        "def extract_and_split_table(chunk, max_rows=10):\n",
        "    lines = chunk.strip().split(\"\\n\")\n",
        "    header, table_rows = None, []\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.match(r'^\\|[-| ]+\\|$', line):\n",
        "            header = lines[i - 1].strip(\"|\").split(\"|\")\n",
        "            header = [h.strip() for h in header]\n",
        "            continue\n",
        "        if header:\n",
        "            row_data = line.strip(\"|\").split(\"|\")\n",
        "            row_data = [cell.strip() for cell in row_data]\n",
        "            table_rows.append(row_data)\n",
        "\n",
        "    # Split table into chunks if too many rows\n",
        "    table_chunks = []\n",
        "    for i in range(0, len(table_rows), max_rows):\n",
        "        chunk_rows = table_rows[i:i + max_rows]\n",
        "        table_chunks.append({\"headers\": header, \"rows\": chunk_rows})\n",
        "\n",
        "    return table_chunks if header and table_rows else None\n",
        "\n",
        "# Function to extract section headers\n",
        "def extract_section_title(header):\n",
        "    match = re.match(r'^(#+)\\s+(.*)', header.strip())\n",
        "    return match.group(2) if match else None\n",
        "\n",
        "# Function to detect table title\n",
        "def detect_table_title(pre_table_text):\n",
        "    lines = pre_table_text.strip().split(\"\\n\")\n",
        "    if lines and len(lines[-1].split()) < 10:  # Assuming a title is a short line before a table\n",
        "        return lines[-1]\n",
        "    return None\n",
        "\n",
        "# Function to split text into chunks of max 400 words with 40-word overlap\n",
        "def split_text(text, section_title, max_words=400, overlap=40):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + max_words, len(words))\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        # Prepend section title to first chunk\n",
        "        if start == 0:\n",
        "            chunk = f\"## {section_title}\\n{chunk}\"\n",
        "        chunks.append(chunk)\n",
        "        start += max_words - overlap\n",
        "    return chunks\n",
        "\n",
        "# Process Markdown\n",
        "sections = re.split(r'^(#+\\s+.*)', markdown_text, flags=re.MULTILINE)\n",
        "final_chunks = []\n",
        "current_section = \"Unknown\"\n",
        "chunk_id = 1\n",
        "\n",
        "for i in range(1, len(sections), 2):\n",
        "    section_title = extract_section_title(sections[i]) or current_section\n",
        "    content = sections[i + 1].strip()\n",
        "    current_section = section_title  # Update current section to maintain hierarchy\n",
        "\n",
        "    table_matches = list(re.finditer(r'(\\|.*\\|\\n\\|[-| ]+\\|\\n(?:\\|.*\\|\\n)+)', content, re.MULTILINE))\n",
        "    last_index = 0\n",
        "\n",
        "    for match in table_matches:\n",
        "        start, end = match.span()\n",
        "        pre_table_text = content[last_index:start].strip()\n",
        "        table_text = match.group(0)\n",
        "        last_index = end\n",
        "\n",
        "        table_title = detect_table_title(pre_table_text)  # Extract table title if present\n",
        "        if pre_table_text:\n",
        "            text_chunks = split_text(pre_table_text, section_title)\n",
        "            for chunk in text_chunks:\n",
        "                final_chunks.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"content\": chunk,\n",
        "                    \"metadata\": {\n",
        "                        \"source\": file_name,\n",
        "                        \"section\": section_title,\n",
        "                        \"position\": chunk_id\n",
        "                    }\n",
        "                })\n",
        "                chunk_id += 1\n",
        "\n",
        "        table_chunks = extract_and_split_table(table_text)\n",
        "        if table_chunks:\n",
        "            for table_chunk in table_chunks:\n",
        "                final_chunks.append({\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"table\": table_chunk,\n",
        "                    \"metadata\": {\n",
        "                        \"source\": file_name,\n",
        "                        \"section\": section_title,\n",
        "                        \"table_title\": table_title,\n",
        "                        \"position\": chunk_id\n",
        "                    }\n",
        "                })\n",
        "                chunk_id += 1\n",
        "\n",
        "    remaining_text = content[last_index:].strip()\n",
        "    if remaining_text:\n",
        "        text_chunks = split_text(remaining_text, section_title)\n",
        "        for chunk in text_chunks:\n",
        "            final_chunks.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"content\": chunk,\n",
        "                \"metadata\": {\n",
        "                    \"source\": file_name,\n",
        "                    \"section\": section_title,\n",
        "                    \"position\": chunk_id\n",
        "                }\n",
        "            })\n",
        "            chunk_id += 1\n",
        "\n",
        "# Save JSON output\n",
        "output_file = \"__________.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(final_chunks, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Chunking completed. JSON saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "H9GrypxEOaiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}